#Shard 分库分表
---
#docs
[数据库分库分表(sharding)系列](http://blog.csdn.net/column/details/sharding.html)

[数据库分库分表(sharding)系列(五) 一种支持自由规划无须数据迁移和修改路由代码的Sharding扩容方案](http://blog.csdn.net/bluishglc/article/details/7970268)
---
#方案
[数据库分库分表(sharding)系列(三) 关于使用框架还是自主开发以及sharding实现层面的考量](http://blog.csdn.net/bluishglc/article/details/7766508)
##sharding逻辑的实现层面
从一个系统的程序架构层面来看，sharding逻辑可以在
* DAO层
* JDBC API层
* 介于DAO与JDBC之间的Spring数据访问封装层(各种spring的template)
* 介于应用服务器与数据库之间的sharding代理服务器四个层面上实现

##在DAO层实现
当团队决定自行实现sharding的时候，DAO层可能是嵌入sharding逻辑的首选位置，因为在这个层面上，每一个DAO的方法都明确地知道需要访问的数据表以及查询参数，借助这些信息可以直接定位到目标shard上，而不必像框架那样需要对SQL进行解析然后再依据配置的规则进行路由。另一个优势是不会受ORM框架的制约。由于现在的大多数应用在数据访问层上会依赖某种ORM框架，而多数的shrading框架往往无法支持或只能支持一种orm框架，这使得在选择和应用框架时受到了很大的制约，而自行实现sharding完全没有这方面的问题，甚至不同的shard使用不同的orm框架都可以在一起协调工作。比如现在的java应用大多使用hibernate，但是当下还没有非常令人满意的基于hibernate的sharding框架，（关于hibernate hards会在下文介绍），因此很多团队会选择自行实现sharding。

简单总结一下，在DAO层自行实现sharding的优势在于：不受ORM框架的制约、实现起来较为简单、易于根据系统特点进行灵活的定制、无需SQL解析和路由规则匹配，性能上表现会稍好一些;劣势在于：有一定的技术门槛，工作量比依靠框架实现要大(反过来看，框架会有学习成本)、不通用，只能在特定系统里工作。当然，在DAO层同样可以通过XML配置或是注解将sharding逻辑抽离到“外部”，形成一套通用的框架. 不过目前还没有出现此类的框架。

##在ORM框架层实现
在ORM框架层实现sharding有两个方向，一个是在实现O-R Mapping的前提下同时提供sharding支持，从而定位为一种分布式的数据访问框架，这一类类型的框架代表就是guzz另一个方向是通过对既有ORM框架进行修改增强来加入sharding机制。此类型的代表产品是hibernate shard. 应该说以hibernate这样主流的地位，行业对于一款面向hibernate的sharding框架的需求是非常迫切的，但是就目前的hibernate shards来看，表现还算不上令人满意，主要是它对使用hibernate的限制过多，比如它对HQL的支持就非常有限。在mybatis方面，目前还没有成熟的相关框架产生。有人提出利用mybatis的插件机制实现sharding,但是遗憾的是，mybatis的插件机制控制不到多数据源的连接层面，另一方面，离开插件层又失去了对sql进行集中解析和路由的机会，因此在mybatis框架上，目前还没有可供借鉴的框架，团队可能要在DAO层或Spring模板类上下功夫了。

##在JDBC API层实现
JDBC API层是很多人都会想到的一个实现sharding的绝佳场所，如果我们能提供一个实现了sharding逻辑的JDBC API实现，那么sharding对于整个应用程序来说就是完全透明的，而这样的实现可以直接作为通用的sharding产品了。但是这种方案的技术门槛和工作量显然不是一般团队能做得来的，因此基本上没有团队会在这一层面上实现sharding,甚至也没有此类的开源产品。笔者知道的只有一款商业产品dbShards采用的是这一方案。

##在介于DAO与JDBC之间的Spring数据访问封装层实现
在springd大行其道的今天，几乎没有哪个java平台上构建的应用不使用spring，在DAO与JDBC之间，spring提供了各种template来管理资源的创建与释放以及与事务的同步，大多数基于spring的应用都会使用template类做为数据访问的入口，这给了我们另一个嵌入sharding逻辑的机会，就是通过提供一个嵌入了sharding逻辑的template类来完成sharding工作.这一方案在效果上与基于JDBC API实现的方案基本一致，同样是对上层代码透明，在进行sharding改造时可以平滑地过度，但它的实现却比基于JDBC API的方式简单，因此成为了不少框架的选择，阿里集团研究院开源的Cobar Client就是这类方案的一种实现。

##在应用服务器与数据库之间通过代理实现
在应用服务器与数据库之间加入一个代理，应用程序向数据发出的数据请求会先通过代理，代理会根据配置的路由规则，对SQL进行解析后路由到目标shard，因为这种方案对应用程序完全透明，通用性好，所以成为了很多sharding产品的选择。在这方面较为知名的产品是mysql官方的代理工具：
* Mysql Proxy
* 一款国人开发的产品:amoeba。
mysql proxy本身并没有实现任何sharding逻辑，它只是作为一种面向mysql数据库的代理，给开发人员提供了一个嵌入sharding逻辑的场所，它使用lua作为编程语言，这对很多团队来说是需要考虑的一个问题。amoeba则是专门实现读写分离与sharding的代理产品，它使用非常简单，不使用任何编程语言，只需要通过xml进行配置。不过amoeba不支持事务(从应用程序发出的包含事务信息的请求到达amoeba时，事务信息会被抹去，因此，即使是单点数据访问也不会有事务存在)一直是个硬伤。当然，这要看产品的定位和设计理念，我们只能说对于那些对事务要求非常高的系统，amoeba是不适合的。

基于代理方式的有MySQL Proxy和Amoeba，基于Hibernate框架的是Hibernate Shards，通过重写spring的ibatis template类是Cobar Client，这些框架各有各的优势与短板，架构师可以在深入调研之后结合项目的实际情况进行选择，但是总的来说，我个人对于框架的选择是持谨慎态度的。一方面多数框架缺乏成功案例的验证，其成熟性与稳定性值得怀疑。另一方面，一些从成功商业产品开源出框架（如阿里和淘宝的一些开源项目）是否适合你的项目是需要架构师深入调研分析的。当然，最终的选择一定是基于项目特点、团队状况、技术门槛和学习成本等综合因素考量确定的。





---
#全局主键生成策略
[数据库分库分表(sharding)系列(二) 全局主键生成策略](http://blog.csdn.net/bluishglc/article/details/7710738)
##UUID
使用UUID作主键是最简单的方案，但是缺点也是非常明显的。由于UUID非常的长，除占用大量存储空间外，最主要的问题是在索引上，在建立索引和基于索引进行查询时都存在性能问题。

##Sequence表
此方案的思路也很简单，在数据库中建立一个Sequence表
由于所有插入任何都需要访问该表，该表很容易成为系统性能瓶颈，同时它也存在单点问题，一旦该表数据库失效，整个应用程序将无法工作。有人提出使用Master-Slave进行主从同步，但这也只能解决单点问题，并不能解决读写比为1:1的访问压力问题。

对每个数据库结点分区段划分ID,以及网上的一些ID生成算法，因为缺少可操作性和实践检验，本文并不推荐。实际上，接下来，我们要介绍的是Fickr使用的一种主键生成方案，这个方案是目前我所知道的最优秀的一个方案，并且经受了实践的检验，可以为大多数应用系统所借鉴。


##一种极为优秀的主键生成策略
flickr开发团队在2010年撰文介绍了flickr使用的一种主键生成测策略，同时表示该方案在flickr上的实际运行效果也非常令人满意，原文连接[http://code.flickr.com/blog/2010/02/08/ticket-servers-distributed-unique-primary-keys-on-the-cheap/](http://code.flickr.com/blog/2010/02/08/ticket-servers-distributed-unique-primary-keys-on-the-cheap/)

建立两台以上的数据库ID生成服务器，每个服务器都有一张记录各表当前ID的Sequence表，但是Sequence中ID增长的步长是服务器的数量，起始值依次错开，这样相当于把ID的生成散列到了每个服务器节点上。例如：如果我们设置两台数据库ID生成服务器，那么就让一台的Sequence表的ID起始值为1,每次增长步长为2,另一台的Sequence表的ID起始值为2,每次增长步长也为2，那么结果就是奇数的ID都将从第一台服务器上生成，偶数的ID都从第二台服务器上生成，这样就将生成ID的压力均匀分散到两台服务器上，同时配合应用程序的控制，当一个服务器失效后，系统能自动切换到另一个服务器上获取ID，从而保证了系统的容错。

细节：
1. flickr的数据库ID生成服务器是专用服务器，服务器上只有一个数据库，数据库中表都是用于生成Sequence的，这也是因为auto-increment-offset和auto-increment-increment这两个数据库变量是数据库实例级别的变量。
2. flickr的方案中表格中的stub字段只是一个char(1) NOT NULL存根字段，并非表名，因此，一般来说，一个Sequence表只有一条纪录，可以同时为多张表生成ID，如果需要表的ID是有连续的，需要为该表单独建立Sequence表。
3. 方案使用了MySQL的LAST_INSERT_ID()函数，这也决定了Sequence表只能有一条记录。
4. 使用REPLACE INTO插入数据，这是很讨巧的作法，主要是希望利用mysql自身的机制生成ID,不仅是因为这样简单，更是因为我们需要ID按照我们设定的方式(初值和步长)来生成。
5. SELECT LAST_INSERT_ID()必须要于REPLACE INTO语句在同一个数据库连接下才能得到刚刚插入的新ID，否则返回的值总是0
6. 该方案中Sequence表使用的是MyISAM引擎，以获取更高的性能，注意：MyISAM引擎使用的是表级别的锁，MyISAM对表的读写是串行的，因此不必担心在并发时两次读取会得到同一个ID(另外，应该程序也不需要同步，每个请求的线程都会得到一个新的connection,不存在需要同步的共享资源)。经过实际对比测试，使用一样的Sequence表进行ID生成，MyISAM引擎要比InnoDB表现高出很多！
7. 可使用纯JDBC实现对Sequence表的操作，以便获得更高的效率，实验表明，即使只使用Spring JDBC性能也不及纯JDBC来得快！

实现该方案，应用程序同样需要做一些处理，主要是两方面的工作：

1. 自动均衡数据库ID生成服务器的访问
2. 确保在某个数据库ID生成服务器失效的情况下，能将请求转发到其他服务器上执行。


##monodb全局ID
IP+pid+时间

MongoDB中数据的基本单元称为文档(Document)。文档是MongoDB的核心概念，多个键极其关联的值有序的放置在一起便是文档。

在一个特定集合内部，需要唯一的标识文档。因此MongoDB中存储的文档都由一个"_id"键，用于完成此功能。这个键的值可以是任意类型的，默认试ObjectId对象。ObjectId对象的生成思路是本文的主题，也是很多分布式系统可以借鉴的思路。

为了考虑分布式，“_id”要求不同的机器都能用全局唯一的同种方法方便的生成它。因此不能使用自增主键（需要多台服务器进行同步，既费时又费力），因此选用了生成ObjectId对象的方法。

ObjectId使用12字节的存储空间，其生成方式如下：
```
|0|1|2|3| 4|5|6 |7|8| 9|10|11|
|时间戳 |机器ID |PID| 计数器   |
```
* 0~3 		
前四个字节时间戳是从标准纪元开始的时间戳，单位为秒，有如下特性：
时间戳与后边5个字节一块，保证秒级别的唯一性；
保证插入顺序大致按时间排序；
隐含了文档创建时间；
时间戳的实际值并不重要，不需要对服务器之间的时间进行同步（因为加上机器ID和进程ID已保证此值唯一，唯一性是ObjectId的最终诉求）。

* 4~6 机器ID
机器ID是服务器主机标识，通常是机器主机名的散列值。

* 7~8 PID
同一台机器上可以运行多个mongod实例，因此也需要加入进程标识符PID。

* 0~8
前9个字节保证了同一秒钟不同机器不同进程产生的ObjectId的唯一性。

* 9~11
后三个字节是一个自动增加的计数器（一个mongod进程需要一个全局的计数器），保证同一秒的ObjectId是唯一的。同一秒钟最多允许每个进程拥有（256^3 = 16777216）个不同的ObjectId。

总结一下：时间戳保证秒级唯一，机器ID保证设计时考虑分布式，避免时钟同步，PID保证同一台服务器运行多个mongod实例时的唯一性，最后的计数器保证同一秒内的唯一性（选用几个字节既要考虑存储的经济性，也要考虑并发性能的上限）。

"_id"既可以在服务器端生成也可以在客户端生成，在客户端生成可以降低服务器端的压力

缺点：字段较长，需要12字节，bigint 8字节


---
#垂直切分的粒度
指的是在做垂直切分时允许几级的关联表放在一个shard里．这个问题对应用程序和sharding实现有着很大的影响．

关联打断地越多，则受影响的join操作越多，应用程序为此做出的妥协就越大，但单表的路由会越简单，与业务的关联性会越小，就越容易使用统一机制处理．在此方向上的极端方案是：打断所有连接，每张表都配有路由规则，可以使用统一机制或框架自动处理．比如amoeba这样的框架，它的路由能且仅能通过SQL的特征（比如某个表的id）进行路由．

反之，若关联打断地越少，则join操作的受到的限制就小，应用程序需要做出的妥协就越小，但是表的路由就会变复杂，与业务的关联性就越大，就越难使用统一机制处理，需要针对每个数据请求单独实现路由．在此方向上的极端方案是：所有表都在一个shard里，也就是没有垂直切分，这样就没有关联被打断．当然这是非常极端的，除非整个数据库很简单，表的数量很少．

实际的粒度掌控需要结合“业务紧密程度”和“表格数据量”两个因素综合考虑，一般来说：
若划归到一起的表格关系紧密，且数据量并不大，增速也非常缓慢，则适宜放在一个shard里，不需要再进行水平切分;
若划归到一起的表格数据量巨大且增速迅猛，则势必要在垂直切分的基础上再进行水平切分，水平切分就意味着原单一shard会被细分成多个更小的shard，每一个shard存在一个主表（即会以该表ID进行散列的表）和多个相之相关的关联表。
总之，垂直切分的粒度在两个相反的方向上呈现优势与劣势并存并相互博弈的局面．架构师需要做的是结合项目的实际情况在两者之间取得收益最大化的平衡．






---
#事务
[分库分表带来的完整性和一致性问题](http://www.cnblogs.com/aigongsi/archive/2013/01/25/2875731.html)
##方案1 XA
使用类似JTA提供的分布式事物机制，也就是说需要相关的数据库提供支持XA的驱动。（ XA 是指由 X/Open 组织提出的分布式交易处理的规范）。这个需要依赖特定的数据库厂商，也是比较简单的方案。毕竟复杂的事务管理都可以通过提供JTA服务的厂商和提供XA驱动的数据库厂商来完成。目前大多数实现了JTA的服务器厂商比较多，比如JBOSS，或者开源的JOTM(Java Open Transaction Manager)——ObjectWeb的一个开源JTA实现。但是引入支持XA的数据库驱动会带来很多潜在的问题，在 《java事务设计策略》里面：在Java事务管理中，常常令人困惑的一个问题是什么时候应该使用XA，什么时候不应使用XA。由于大多数商业应用服务器执行单阶段提交（one-phase commit）操作，性能下降并非一个值得考虑的问题。然而，非必要性的在您的应用中引入XA数据库驱动，会导致不可预料的后果与错误，特别是在使用本地事务模型（Local Transaction Model）时。因此，一般来说在您不需要XA的时候，应该尽量避免使用它。”  所以这个是一个可选的方案，也是最简单的一个方案

##方案2
建立一张文件批次表（放在一个独立的数据库里面），保存待处理的文件批次信息（不是明细数据，简单说的就是要处理的文件名和所在路径），在每次处理文件数据的时候，先往表里面插入一条文件批次信息，并且设置文件的状态为初始状态，在文件中的数据全部成功的保存到三个分库里面之后，在更新文件的批次状态为成功。如果保存到分库的过程中出现异常，文件批次的状态还是初始状态。而后台启动一个定时机制，定时去扫描文件批次状态，如果发现是初始状态，就重新执行文件的导入操作，直到文件完全导入成功。这个方案看起来没有问题，但是可能存在重复导入的情况，比如批次导入到第一个分库成功了，后面两个库失败了，重新导入的话，可能会重复把数据重复导入第一个分库。我们可以在导入之间进行判断，如果导入过，就不进行导入，但是极端的情况，我们无法判断数据是否导入过，也是一个有缺陷的方案，并且如果每次导入之前，都进行数据是否导入的操作，性能会有一些影响。我们也可以通过异常恢复机制来进行，如果发现文件导入失败了，我们删除已经导入入库的流水，但是这也引入了错误处理带来的一致性问题，比如我们已经导入成功2个分库的数据，在导入第三个分库失败的情况下，要删除掉前面两个分库的数据，这也没有办法保证是一致的。

##方案3
第2个方案的基础上，可以继续加以优化。首先我们保留第二个方案的文件批次信息表和子文件批次信息表，而且我们必须把这两个表放在同一个库里面（这里假设分配到主库），保证我们拆分任务时的一致性。然后在各个分库里面，我们建立一张各个分库的子文件批次表。这个表模型基本上是和主库的子文件批次信息表一样。当拆分任务的时候，先保证主库数据的完整性，也就是产生了一条文件批次信息记录和三条子文件批次记录，然后把这三条子文件批次信息分别复制到对应的分库中的子文件批次信息表里面，然后更新主库的子文件批次信息状态为“已同步”。当然，这个过程是无法保证一致性的。解决方案启动一个定时任务，定期的把主库重点的子文件批次表信息中初始状态的记录 同步到各个分库的子文件批次表里面，这里面可能导致两种情况

1 分库子批次信息表已经存在相同的信息（这个可以通过唯一性主键保证），说明已经同步，直接更新主库的子文件批次信息状态为 “已经同步”

2 分库子批次信息不存在，则往对应的分库insert一条数据，然后更新主库的子文件批次信息状态为 “已经同步”


##方案4 两阶段提交
优势：
1. 基于两阶段提交，最大限度地保证了跨数据库操作的“原子性”，是分布式系统下最严格的事务实现方式。
2. 实现简单，工作量小。由于多数应用服务器以及一些独立的分布式事务协调器做了大量的封装工作，使得项目中引入分布式事务的难度和工作量基本上可以忽略不计。
劣势：
系统“水平”伸缩的死敌。基于两阶段提交的分布式事务在提交事务时需要在多个节点之间进行协调,最大限度地推后了提交事务的时间点，客观上延长了事务的执行时间，这会导致事务在访问共享资源时发生冲突和死锁的概率增高，随着数据库节点的增多，这种趋势会越来越严重，从而成为系统在数据库层面上水平伸缩的"枷锁"， 这是很多Sharding系统不采用分布式事务的主要原因。

两阶段提交的有三个重要的子操作：准备提交，提交，回滚。
继续拿文件导入来举例子，各个分库作为一个事务参与者 ， 我们需要设计各个分库的准备提交操作，提交，回滚操作。

准备提交阶段：各个分库可以把要处理的文件明细保存到一张临时表里面，并且记住这一次事务中上下文信息。

提交阶段：把这一次事务上下文中对应的临时表数据同步到对应的明细表中

回滚阶段：删除本次事务相关的临时表流水信息。

##基于Best Efforts 1PC模式的事务
与分布式事务采用的两阶段提交不同，Best Efforts 1PC模式采用的是一阶段端提交，牺牲了事务在某些特殊情况(当机、网络中断等)下的安全性，却获得了良好的性能，特别是消除了对水平伸缩的桎酷。
[Distributed transactions in Spring, with and without XA](http://www.javaworld.com/javaworld/jw-01-2009/jw-01-spring-transactions.html?page=5)
一文对Best Efforts 1PC模式进行了详细的说明，该文提供的Demo代码更是直接给出了在Spring环境下实现一阶段提交的多数据源事务管理示例。
不过需要注意的是，原示例是基于spring 3.0之前的版本，如果你使用spring 3.0+,会得到如下错误：
java.lang.IllegalStateException: Cannot activate transaction synchronization - already active，如果使用spring 3.0+，你需要参考spring-data-neo4j的实现。
鉴于Best Efforts1PC模式的性能优势，以及相对简单的实现方式，它被大多数的sharding框架和项目采用。

##事务补偿机制
对于那些对性能要求很高，但对一致性要求并不高的系统，往往并不苛求系统的实时一致性，只要在一个允许的时间周期内达到最终一致性即可，这使得事务补偿机制成为一种可行的方案。
事务补偿机制最初被提出是在“长事务”的处理中，但是对于分布式系统确保一致性也有很好的参考意义。笼统地讲，与事务在执行中发生错误后立即回滚的方式不同，事务补偿是一种事后检查并补救的措施，它只期望在一个容许时间周期内得到最终一致的结果就可以了。
事务补偿的实现与系统业务紧密相关，并没有一种标准的处理方式。
一些常见的实现方式有：
对数据进行对帐检查;
基于日志进行比对;
定期同标准数据来源进行同步，等等。



































